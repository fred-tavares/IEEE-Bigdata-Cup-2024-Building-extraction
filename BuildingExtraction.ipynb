{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":76972,"databundleVersionId":8669583,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"############################ PRE PROCESSAMANTOOOOOOOOO\"\"\"\"\"\nfrom PIL import Image, ImageEnhance\nimport os\n\ninput_dirs = [\n    \"/kaggle/input/building-extraction-generalization-2024/train\",\n    \"/kaggle/input/building-extraction-generalization-2024/val\",\n    \"/kaggle/input/building-extraction-generalization-2024/test\",\n]\noutput_root = \"/kaggle/working/processed\"\ngamma = 0.75\nexponent = (1.0 / gamma) if gamma < 1 else gamma\ngamma_table_8bit = [int(((i / 255.0) ** exponent) * 255.0) for i in range(256)]\ngamma_table_16bit = None\n\nfor split_dir in input_dirs:\n    if not os.path.exists(split_dir):\n        continue\n    split_name = os.path.basename(split_dir)\n    for root, dirs, files in os.walk(split_dir):\n        rel_path = os.path.relpath(root, split_dir)\n        output_dir = os.path.join(output_root, split_name, rel_path) if rel_path != \".\" else os.path.join(output_root, split_name)\n        os.makedirs(output_dir, exist_ok=True)\n        for filename in files:\n            fname_lower = filename.lower()\n            if not (fname_lower.endswith(\".jpg\") or fname_lower.endswith(\".jpeg\") or \n                    fname_lower.endswith(\".png\") or fname_lower.endswith(\".tif\") or fname_lower.endswith(\".tiff\")):\n                continue\n            input_path = os.path.join(root, filename)\n            output_path = os.path.join(output_dir, filename)\n            try:\n                with Image.open(input_path) as im:\n                    mode = im.mode\n                    if mode == \"P\":\n                        im = im.convert(\"RGB\")\n                        mode = im.mode\n                    if mode == \"RGB\":\n                        r, g, b = im.split()\n                        r = r.point(gamma_table_8bit)\n                        g = g.point(gamma_table_8bit)\n                        b = b.point(gamma_table_8bit)\n                        im = Image.merge(\"RGB\", (r, g, b))\n                    elif mode == \"RGBA\":\n                        r, g, b, a = im.split()\n                        r = r.point(gamma_table_8bit)\n                        g = g.point(gamma_table_8bit)\n                        b = b.point(gamma_table_8bit)\n                        rgb_im = Image.merge(\"RGB\", (r, g, b))\n                        rgb_im = ImageEnhance.Contrast(rgb_im).enhance(1.3)\n                        r, g, b = rgb_im.split()\n                        im = Image.merge(\"RGBA\", (r, g, b, a))\n                        im.save(output_path)\n                        continue\n                    elif mode == \"L\":\n                        im = im.point(gamma_table_8bit)\n                    elif mode.startswith(\"I;16\"):\n                        if gamma_table_16bit is None:\n                            gamma_table_16bit = [int(((i / 65535.0) ** exponent) * 65535.0) for i in range(65536)]\n                        im = im.point(gamma_table_16bit, mode)\n                    im = ImageEnhance.Contrast(im).enhance(1.3)\n                    im.save(output_path)\n            except Exception as e:\n                print(f\"Failed to process {input_path}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:11:42.549971Z","iopub.execute_input":"2025-10-06T14:11:42.550752Z","iopub.status.idle":"2025-10-06T14:12:57.248586Z","shell.execute_reply.started":"2025-10-06T14:11:42.550722Z","shell.execute_reply":"2025-10-06T14:12:57.247727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# MODELOOOO \n#1. Register COCO-format datasets\n# --- Pillow compatibility shim: define aliases Detectron2 expects ---\nimport json\n\nfrom PIL import Image as _Image\ntry:\n    from PIL.Image import Resampling as _Resampling\nexcept Exception:\n    _Resampling = None\n\ndef _alias(dst, src):\n    if not hasattr(_Image, dst):\n        if hasattr(_Image, src):\n            setattr(_Image, dst, getattr(_Image, src))\n        elif _Resampling is not None and hasattr(_Resampling, src):\n            setattr(_Image, dst, getattr(_Resampling, src))\n\n# Map missing names used by some Detectron2 versions\n_alias(\"LINEAR\", \"BILINEAR\")      # use BILINEAR for \"LINEAR\"\n_alias(\"CUBIC\", \"BICUBIC\")        # use BICUBIC for \"CUBIC\"\n_alias(\"ANTIALIAS\", \"LANCZOS\")    # ANTIALIAS → LANCZOS in Pillow>=10\n\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()  # call this before creating the trainer\n\n\n!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@4a5e6d79e626837a0317195131afaca64b3f4e2d'\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.data.datasets import register_coco_instances\n\nTRAIN_IMG_DIR = \"/kaggle/working/processed/train/\"\nVAL_IMG_DIR   = \"/kaggle/working/processed/val/\"\n#TRAIN_IMG_DIR = \"/kaggle/input/building-extraction-generalization-2024/train\"\n#VAL_IMG_DIR   = \"/kaggle/input/building-extraction-generalization-2024/val\"\n\nTRAIN_JSON   = \"/kaggle/input/building-extraction-generalization-2024/train/train.json\"\nVAL_JSON     = \"/kaggle/input/building-extraction-generalization-2024/val/val.json\"\ndef reregister_coco(name, json_file, image_root):\n    # If the dataset was registered earlier in this process, clear it first\n    if name in DatasetCatalog.list():\n        DatasetCatalog.remove(name)\n    try:\n        MetadataCatalog.remove(name)  # avoid stale metadata\n    except KeyError:\n        pass\n    register_coco_instances(name, {}, json_file, image_root)\n\nreregister_coco(\"train_dataset\", TRAIN_JSON, TRAIN_IMG_DIR)\nreregister_coco(\"val_dataset\",   VAL_JSON,   VAL_IMG_DIR)\n\n\n# 2. Configure Mask R-CNN and train\nfrom detectron2.config import get_cfg\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultTrainer\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"train_dataset\",)\ncfg.DATASETS.TEST  = (\"val_dataset\",)  # for evaluation\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1      # one class (building)\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.MAX_ITER = 1892*6 #10 # adjust as needed\n# Train the model\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()\n\n# 3. (Optional) Validate on the val set\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.engine import DefaultPredictor\n# We can evaluate using Detectron2’s COCOEvaluator (computes AP for instance masks):contentReference[oaicite:4]{index=4}.\nevaluator = COCOEvaluator(\"val_dataset\", cfg, False, output_dir=\"./output\")\nevaluator.reset()  # makes _coco_api\nevaluator._coco_api.dataset.setdefault(\"info\", {})\nval_loader = build_detection_test_loader(cfg, \"val_dataset\")\n# Run inference on val and compute metrics\nmetrics = inference_on_dataset(trainer.model, val_loader, evaluator)\nprint(\"Validation metrics:\", metrics)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-07T00:19:10.234380Z","iopub.execute_input":"2025-10-07T00:19:10.234742Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/detectron2.git@4a5e6d79e626837a0317195131afaca64b3f4e2d\n  Cloning https://github.com/facebookresearch/detectron2.git (to revision 4a5e6d79e626837a0317195131afaca64b3f4e2d) to /tmp/pip-req-build-multm04u\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-multm04u\n  Running command git rev-parse -q --verify 'sha^4a5e6d79e626837a0317195131afaca64b3f4e2d'\n  Running command git fetch -q https://github.com/facebookresearch/detectron2.git 4a5e6d79e626837a0317195131afaca64b3f4e2d\n  Running command git checkout -q 4a5e6d79e626837a0317195131afaca64b3f4e2d\n  Resolved https://github.com/facebookresearch/detectron2.git to commit 4a5e6d79e626837a0317195131afaca64b3f4e2d\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.7.2)\nRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.10)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\nRequirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.8)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\nRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\nRequirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\nRequirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.9)\nRequirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\nRequirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (1.3.2)\nRequirement already satisfied: black in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (25.9.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (25.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.2.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.2.1)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (1.1.0)\nRequirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (0.12.1)\nRequirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.8)\nRequirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (0.1.10)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.73.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8.2)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2024.2.0)\n[10/07 00:19:20 d2.engine.defaults]: Model:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n[10/07 00:19:21 d2.data.datasets.coco]: Loading /kaggle/input/building-extraction-generalization-2024/train/train.json takes 1.43 seconds.\nWARNING [10/07 00:19:21 d2.data.datasets.coco]: \nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n\n[10/07 00:19:21 d2.data.datasets.coco]: Loaded 3784 images in COCO format from /kaggle/input/building-extraction-generalization-2024/train/train.json\n[10/07 00:19:22 d2.data.build]: Removed 0 images with no usable annotations. 3784 images left.\n[10/07 00:19:22 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n[10/07 00:19:22 d2.data.build]: Using training sampler TrainingSampler\n[10/07 00:19:22 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n[10/07 00:19:22 d2.data.common]: Serializing 3784 elements to byte tensors and concatenating them all ...\n[10/07 00:19:22 d2.data.common]: Serialized dataset takes 10.97 MiB\nWARNING [10/07 00:19:22 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n[10/07 00:19:23 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n[10/07 00:19:23 d2.engine.train_loop]: Starting training from iteration 0\n[10/07 00:19:33 d2.utils.events]:  eta: 1:32:44  iter: 19  total_loss: 3.146  loss_cls: 0.621  loss_box_reg: 0.425  loss_mask: 0.6939  loss_rpn_cls: 1.067  loss_rpn_loc: 0.2478  time: 0.4858  data_time: 0.0233  lr: 4.9953e-06  max_mem: 2808M\n[10/07 00:19:43 d2.utils.events]:  eta: 1:34:52  iter: 39  total_loss: 2.966  loss_cls: 0.6033  loss_box_reg: 0.4731  loss_mask: 0.6908  loss_rpn_cls: 0.7812  loss_rpn_loc: 0.2935  time: 0.5014  data_time: 0.0063  lr: 9.9902e-06  max_mem: 2808M\n[10/07 00:19:53 d2.utils.events]:  eta: 1:34:57  iter: 59  total_loss: 2.664  loss_cls: 0.5768  loss_box_reg: 0.4911  loss_mask: 0.6831  loss_rpn_cls: 0.6427  loss_rpn_loc: 0.3607  time: 0.5000  data_time: 0.0060  lr: 1.4985e-05  max_mem: 2808M\n[10/07 00:20:03 d2.utils.events]:  eta: 1:33:58  iter: 79  total_loss: 2.29  loss_cls: 0.5399  loss_box_reg: 0.5329  loss_mask: 0.6716  loss_rpn_cls: 0.2933  loss_rpn_loc: 0.2219  time: 0.4939  data_time: 0.0062  lr: 1.998e-05  max_mem: 2808M\n[10/07 00:20:13 d2.utils.events]:  eta: 1:34:01  iter: 99  total_loss: 2.234  loss_cls: 0.5019  loss_box_reg: 0.4744  loss_mask: 0.6617  loss_rpn_cls: 0.3225  loss_rpn_loc: 0.2633  time: 0.4934  data_time: 0.0062  lr: 2.4975e-05  max_mem: 2808M\n[10/07 00:20:23 d2.utils.events]:  eta: 1:33:57  iter: 119  total_loss: 2.48  loss_cls: 0.5015  loss_box_reg: 0.5433  loss_mask: 0.641  loss_rpn_cls: 0.3574  loss_rpn_loc: 0.3085  time: 0.4957  data_time: 0.0059  lr: 2.997e-05  max_mem: 2808M\n[10/07 00:20:33 d2.utils.events]:  eta: 1:33:47  iter: 139  total_loss: 2.111  loss_cls: 0.4833  loss_box_reg: 0.5703  loss_mask: 0.6264  loss_rpn_cls: 0.2607  loss_rpn_loc: 0.272  time: 0.4958  data_time: 0.0060  lr: 3.4965e-05  max_mem: 2808M\n[10/07 00:20:43 d2.utils.events]:  eta: 1:33:37  iter: 159  total_loss: 2.22  loss_cls: 0.5005  loss_box_reg: 0.6134  loss_mask: 0.6065  loss_rpn_cls: 0.2287  loss_rpn_loc: 0.275  time: 0.4963  data_time: 0.0060  lr: 3.996e-05  max_mem: 2808M\n[10/07 00:20:52 d2.utils.events]:  eta: 1:33:24  iter: 179  total_loss: 2.127  loss_cls: 0.4176  loss_box_reg: 0.4812  loss_mask: 0.5782  loss_rpn_cls: 0.2229  loss_rpn_loc: 0.2602  time: 0.4941  data_time: 0.0058  lr: 4.4955e-05  max_mem: 2808M\n[10/07 00:21:02 d2.utils.events]:  eta: 1:33:14  iter: 199  total_loss: 2.22  loss_cls: 0.4427  loss_box_reg: 0.549  loss_mask: 0.5559  loss_rpn_cls: 0.2657  loss_rpn_loc: 0.2992  time: 0.4948  data_time: 0.0060  lr: 4.995e-05  max_mem: 2808M\n[10/07 00:21:12 d2.utils.events]:  eta: 1:33:03  iter: 219  total_loss: 2.023  loss_cls: 0.4109  loss_box_reg: 0.5194  loss_mask: 0.5384  loss_rpn_cls: 0.2312  loss_rpn_loc: 0.2814  time: 0.4943  data_time: 0.0057  lr: 5.4945e-05  max_mem: 2808M\n[10/07 00:21:22 d2.utils.events]:  eta: 1:32:54  iter: 239  total_loss: 1.942  loss_cls: 0.4088  loss_box_reg: 0.5071  loss_mask: 0.5181  loss_rpn_cls: 0.2222  loss_rpn_loc: 0.2338  time: 0.4936  data_time: 0.0057  lr: 5.994e-05  max_mem: 2808M\n[10/07 00:21:32 d2.utils.events]:  eta: 1:32:47  iter: 259  total_loss: 1.813  loss_cls: 0.3844  loss_box_reg: 0.5744  loss_mask: 0.4834  loss_rpn_cls: 0.212  loss_rpn_loc: 0.243  time: 0.4948  data_time: 0.0060  lr: 6.4935e-05  max_mem: 2808M\n[10/07 00:21:42 d2.utils.events]:  eta: 1:32:33  iter: 279  total_loss: 1.843  loss_cls: 0.3851  loss_box_reg: 0.5092  loss_mask: 0.4475  loss_rpn_cls: 0.1717  loss_rpn_loc: 0.2497  time: 0.4937  data_time: 0.0064  lr: 6.993e-05  max_mem: 2808M\n[10/07 00:21:51 d2.utils.events]:  eta: 1:32:18  iter: 299  total_loss: 1.931  loss_cls: 0.3686  loss_box_reg: 0.5889  loss_mask: 0.422  loss_rpn_cls: 0.1829  loss_rpn_loc: 0.2697  time: 0.4932  data_time: 0.0060  lr: 7.4925e-05  max_mem: 2808M\n[10/07 00:22:01 d2.utils.events]:  eta: 1:32:06  iter: 319  total_loss: 1.816  loss_cls: 0.3467  loss_box_reg: 0.5454  loss_mask: 0.4107  loss_rpn_cls: 0.1734  loss_rpn_loc: 0.2616  time: 0.4923  data_time: 0.0059  lr: 7.992e-05  max_mem: 2808M\n[10/07 00:22:11 d2.utils.events]:  eta: 1:31:40  iter: 339  total_loss: 1.84  loss_cls: 0.3399  loss_box_reg: 0.5912  loss_mask: 0.4015  loss_rpn_cls: 0.2045  loss_rpn_loc: 0.2529  time: 0.4919  data_time: 0.0061  lr: 8.4915e-05  max_mem: 2808M\n[10/07 00:22:20 d2.utils.events]:  eta: 1:31:19  iter: 359  total_loss: 1.522  loss_cls: 0.3153  loss_box_reg: 0.4613  loss_mask: 0.3803  loss_rpn_cls: 0.1491  loss_rpn_loc: 0.1989  time: 0.4911  data_time: 0.0062  lr: 8.991e-05  max_mem: 2808M\n[10/07 00:22:30 d2.utils.events]:  eta: 1:31:12  iter: 379  total_loss: 1.606  loss_cls: 0.3135  loss_box_reg: 0.5089  loss_mask: 0.3676  loss_rpn_cls: 0.1607  loss_rpn_loc: 0.2352  time: 0.4913  data_time: 0.0064  lr: 9.4905e-05  max_mem: 2808M\n[10/07 00:22:40 d2.utils.events]:  eta: 1:31:06  iter: 399  total_loss: 1.806  loss_cls: 0.3229  loss_box_reg: 0.5933  loss_mask: 0.347  loss_rpn_cls: 0.1784  loss_rpn_loc: 0.3058  time: 0.4914  data_time: 0.0059  lr: 9.99e-05  max_mem: 2808M\n[10/07 00:22:50 d2.utils.events]:  eta: 1:30:49  iter: 419  total_loss: 1.625  loss_cls: 0.3211  loss_box_reg: 0.4862  loss_mask: 0.3439  loss_rpn_cls: 0.1538  loss_rpn_loc: 0.2399  time: 0.4912  data_time: 0.0057  lr: 0.0001049  max_mem: 2808M\n[10/07 00:23:00 d2.utils.events]:  eta: 1:30:42  iter: 439  total_loss: 1.697  loss_cls: 0.3221  loss_box_reg: 0.5958  loss_mask: 0.3492  loss_rpn_cls: 0.1543  loss_rpn_loc: 0.284  time: 0.4919  data_time: 0.0061  lr: 0.00010989  max_mem: 2808M\n[10/07 00:23:10 d2.utils.events]:  eta: 1:30:32  iter: 459  total_loss: 1.505  loss_cls: 0.2906  loss_box_reg: 0.4037  loss_mask: 0.332  loss_rpn_cls: 0.1479  loss_rpn_loc: 0.2132  time: 0.4916  data_time: 0.0059  lr: 0.00011489  max_mem: 2808M\n[10/07 00:23:20 d2.utils.events]:  eta: 1:30:19  iter: 479  total_loss: 1.659  loss_cls: 0.305  loss_box_reg: 0.5859  loss_mask: 0.3319  loss_rpn_cls: 0.1836  loss_rpn_loc: 0.2813  time: 0.4918  data_time: 0.0059  lr: 0.00011988  max_mem: 2808M\n[10/07 00:23:29 d2.utils.events]:  eta: 1:30:06  iter: 499  total_loss: 1.536  loss_cls: 0.3239  loss_box_reg: 0.5357  loss_mask: 0.3414  loss_rpn_cls: 0.1297  loss_rpn_loc: 0.2099  time: 0.4917  data_time: 0.0059  lr: 0.00012488  max_mem: 2808M\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#### TEST e .CSV\n\nimport os\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# --- Build predictor from your trained weights ---\nckpt_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\nif not os.path.exists(ckpt_path):\n    with open(os.path.join(cfg.OUTPUT_DIR, \"last_checkpoint\")) as f:\n        ckpt_path = f.read().strip()\n\ncfg_pred = cfg.clone()\ncfg_pred.MODEL.WEIGHTS = ckpt_path\ncfg_pred.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\npredictor = DefaultPredictor(cfg_pred)   # <— use cfg_pred, not cfg\n\n# --- Inference on test and write polygons as strings ---\nimport os, cv2, numpy as np, pandas as pd, ast\n\nTEST_IMG_DIR = \"/kaggle/working/processed/test/image\"\nresults = []\n\ntest_images = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(\".tif\")])\n\nfor img_name in test_images:\n    im = cv2.imread(os.path.join(TEST_IMG_DIR, img_name))\n    outputs = predictor(im)\n\n    polygons = []   # list of polygons; each polygon = list of (x, y) ints\n\n    if \"instances\" in outputs and len(outputs[\"instances\"]) > 0:\n        masks = outputs[\"instances\"].pred_masks.cpu().numpy()\n\n        for mask in masks:\n            mask_uint8 = (mask.astype(np.uint8) * 255)\n\n            # External boundaries only; reduce point count\n            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for cnt in contours:\n                if cnt.shape[0] < 3:  # need at least a triangle\n                    continue\n\n                # Optional: further simplify if boundaries are too dense\n                # epsilon = 0.002 * cv2.arcLength(cnt, True)\n                # cnt = cv2.approxPolyDP(cnt, epsilon, True)\n\n                pts = cnt.squeeze(1).tolist()  # [[x,y], [x,y], ...]\n                if len(pts) < 3:\n                    continue\n\n                # ensure closed by repeating first point\n                if pts[0] != pts[-1]:\n                    pts.append(pts[0])\n\n                # convert to list of tuples [(x,y), ...] with ints\n                poly = [(int(x), int(y)) for x, y in pts]\n                polygons.append(poly)\n\n    # IMPORTANT: Coordinates must be a STRING representing list-of-polygons\n    # Example for one image: \"[[(x0,y0),(x1,y1),...], [(x0,y0),(x1,y1),...]]\"\n    coord_str = str(polygons) if len(polygons) > 0 else \"[]\"\n\n    image_id = int(os.path.splitext(img_name)[0])  # \"0001.jpg\" -> 1\n    results.append({\"ImageID\": image_id, \"Coordinates\": coord_str})\n\nsub_df = pd.DataFrame(results, columns=[\"ImageID\", \"Coordinates\"])\n\n\n# verify that each Coordinates cell can be parsed as a Python literal list\nfor s in sub_df[\"Coordinates\"]:\n    try:\n        polys = ast.literal_eval(s)\n        assert isinstance(polys, list)\n        # polys may be [] or list of polygons (lists/tuples)\n    except Exception as e:\n        raise ValueError(f\"Bad Coordinates cell: {s[:80]}... -> {e}\")\n\nsub_df.to_csv(\"submissionH.csv\", index=False)\n\n\n# sanity checks to avoid \"metric error\"\nassert sub_df.shape[0] == 1000, \"Submission must have exactly 1000 rows.\"\nassert sub_df[\"ImageID\"].is_unique, \"Duplicate ImageID rows.\"\nassert sub_df[\"ImageID\"].min() >= 0, \"ImageID should start at 1.\"\nassert sub_df[\"ImageID\"].max() <= 1000, \"ImageID should not exceed number of test images.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T00:09:47.947272Z","iopub.execute_input":"2025-10-07T00:09:47.947628Z","iopub.status.idle":"2025-10-07T00:12:16.623745Z","shell.execute_reply.started":"2025-10-07T00:09:47.947585Z","shell.execute_reply":"2025-10-07T00:12:16.622829Z"}},"outputs":[{"name":"stdout","text":"[10/07 00:09:48 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"#VISUALLLLLIZAÇÂAO\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n\n\n\n\nVAL_IMG_DIR   = \"/kaggle/working/processed/val/image\"\n\nimport os, cv2, json, numpy as np, pandas as pd\nfrom detectron2.engine import DefaultPredictor\n\n# --- Build a predictor that uses your trained weights ---\ncfg_pred = cfg.clone()\n# Try to use the final trained checkpoint; fall back to last checkpoint; else keep the existing weights.\ntrained_path = os.path.join(cfg_pred.OUTPUT_DIR, \"model_final.pth\")\nif not os.path.exists(trained_path):\n    last_ckpt_file = os.path.join(cfg_pred.OUTPUT_DIR, \"last_checkpoint\")\n    if os.path.exists(last_ckpt_file):\n        with open(last_ckpt_file, \"r\") as f:\n            ckpt_name = f.read().strip()\n        maybe_path = os.path.join(cfg_pred.OUTPUT_DIR, ckpt_name)\n        if os.path.exists(maybe_path):\n            trained_path = maybe_path\n\nif os.path.exists(trained_path):\n    cfg_pred.MODEL.WEIGHTS = trained_path  # use trained weights\nelse:\n    print(f\"[Warning] Trained weights not found at {trained_path}. Using cfg.MODEL.WEIGHTS as-is.\")\n\ncfg_pred.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # adjust if needed\npredictor = DefaultPredictor(cfg_pred)\n\n# --- Take first 10 validation images (sorted) ---\nVALID_EXTS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\")\nval_images = sorted([f for f in os.listdir(VAL_IMG_DIR) if f.lower().endswith(VALID_EXTS)])[:10]\n\n# --- Output dirs & collectors ---\noverlay_dir = \"/kaggle/working/val_overlays\"\nos.makedirs(overlay_dir, exist_ok=True)\nval_results = []  # to store polygons like your test loop\n\nfor img_name in val_images:\n    img_path = os.path.join(VAL_IMG_DIR, img_name)\n    im_bgr = cv2.imread(img_path)\n    if im_bgr is None:\n        print(f\"[Skip] Could not read {img_path}\")\n        continue\n\n    outputs = predictor(im_bgr)\n    inst = outputs[\"instances\"].to(\"cpu\")\n\n    masks = inst.pred_masks.numpy() if inst.has(\"pred_masks\") else []\n    # Build polygons like your test loop\n    polygons = []\n    for mask in masks:\n        mask_uint8 = mask.astype(\"uint8\")\n        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        for cnt in contours:\n            pts = cnt.squeeze(1)\n            if pts.size == 0:\n                continue\n            poly = [(int(x), int(y)) for x, y in pts]\n            if poly and poly[0] != poly[-1]:\n                poly.append(poly[0])  # close polygon\n            polygons.append(poly)\n\n    # Save polygons using same string format you used\n    coord_str = str(polygons) if polygons else \"[]\"\n\n    # Derive a numeric ID if filename is numeric; else keep the name\n    base, _ = os.path.splitext(img_name)\n    try:\n        image_id = int(base)\n    except ValueError:\n        image_id = base\n\n    val_results.append({\"ImageID\": image_id, \"Coordinates\": coord_str})\n\n    # --- Make red overlay (fill + contour) ---\n    overlay = im_bgr.copy()\n    # Fill red on mask pixels (BGR -> red is (0,0,255))\n    if len(masks) > 0:\n        union_mask = np.any(masks, axis=0)\n        overlay[union_mask] = (0, 0, 255)\n\n    # Blend with original\n    alpha = 0.5\n    vis = cv2.addWeighted(overlay, alpha, im_bgr, 1 - alpha, 0)\n\n    # Optional: red contour outlines for crisp edges\n    for mask in masks:\n        cnts, _ = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(vis, cnts, -1, (0, 0, 255), 2)\n\n    # Save visualization next to working dir\n    out_path = os.path.join(overlay_dir, f\"val_overlay_{img_name}\")\n    cv2.imwrite(out_path, vis)\n\n# Save polygons for these 10 val images (handy for debugging)\nval_df = pd.DataFrame(val_results)\nval_df.to_csv(\"/kaggle/working/val_overlays/val_first10_footprintsB.csv\", index=False)\n\nprint(f\"Saved {len(val_images)} overlays to: {overlay_dir}\")\nprint(\"Saved polygons to: /kaggle/working/val_first10_footprints.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T20:22:50.583278Z","iopub.execute_input":"2025-10-06T20:22:50.583619Z","iopub.status.idle":"2025-10-06T20:22:53.689909Z","shell.execute_reply.started":"2025-10-06T20:22:50.583576Z","shell.execute_reply":"2025-10-06T20:22:53.689008Z"}},"outputs":[{"name":"stdout","text":"[10/06 20:22:51 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\nSaved 10 overlays to: /kaggle/working/val_overlays\nSaved polygons to: /kaggle/working/val_first10_footprints.csv\n","output_type":"stream"}],"execution_count":62}]}